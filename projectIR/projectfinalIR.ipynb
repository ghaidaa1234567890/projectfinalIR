{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f71a0726-ed7c-4301-9dc9-f4c328719056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import tkinter as tk\n",
    "from tkinter import scrolledtext\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import difflib\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "#The first question of the project is how to clean the data set and query processing\n",
    "class TextCleaning:\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        text = re.sub(r'\\b(about|above|across|after|against|along|amid|among|an|and|as|at|be|because|been|before|being|below|beneath|beside|between|beyond|but|by|down|during|for|from|in|inside|into|like|near|of|off|on|onto|out|over|past|since|throughout|to|toward|underneath|until|up|upon|with)\\b', '', text)\n",
    "        text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "        text = ' '.join([word for word in word_tokenize(text) if word.lower() not in self.stop_words])\n",
    "        return text.strip()\n",
    "\n",
    "def load_and_clean_data(input_path):\n",
    "    with open(input_path, 'r') as file:\n",
    "        data = file.readlines()\n",
    "\n",
    "    cleaning_class = TextCleaning()\n",
    "    cleaned_data = []\n",
    "    raw_data = []\n",
    "\n",
    "    for one_text in data:\n",
    "        cleaned_text = cleaning_class.clean_text(one_text)\n",
    "        if cleaned_text:\n",
    "            cleaned_data.append(cleaned_text)\n",
    "            raw_data.append(one_text)\n",
    "\n",
    "    return cleaned_data, raw_data\n",
    "#The second question: Representing data in the form of vectors\n",
    "def prepare_data(cleaned_data, raw_data):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(cleaned_data)\n",
    "\n",
    "    ids_data = [json.loads(line).get('_id', f\"document{i}\") for i, line in enumerate(raw_data)]\n",
    "\n",
    "    return vectorizer, tfidf_matrix, ids_data\n",
    "\n",
    "relevant_docs = {}\n",
    "#A class containing the interface and buttons\n",
    "\n",
    "class Application(tk.Tk):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.title(\"Information Retrieval System\")\n",
    "        self.geometry(\"700x700\")\n",
    "\n",
    "        self.query_label = tk.Label(self, text=\"Enter your query:\")\n",
    "        self.query_label.pack()\n",
    "\n",
    "        self.query_entry = tk.Entry(self)\n",
    "        self.query_entry.pack()\n",
    "\n",
    "        self.search_button = tk.Button(self, text=\"Search\", command=self.search)\n",
    "        self.search_button.pack()\n",
    "#Additional order button\n",
    "        self.suggestions_button = tk.Button(self, text=\"Get Suggestions\", command=self.get_suggestions)\n",
    "        self.suggestions_button.pack()\n",
    "\n",
    "        self.results_text = scrolledtext.ScrolledText(self, width=70, height=15)\n",
    "        self.results_text.pack()\n",
    "\n",
    "        self.metrics_text = tk.Text(self, height=10, width=70)\n",
    "        self.metrics_text.pack()\n",
    "\n",
    "        self.queries_searched = []\n",
    "        self.precision_list = []\n",
    "        self.recall_list = []\n",
    "        self.average_precision_list = []\n",
    "        self.reciprocal_rank_list = []\n",
    "\n",
    "        self.dataset_var = tk.StringVar(self)\n",
    "        self.dataset_var.set(\"Select Dataset\")\n",
    "        self.dataset_menu = tk.OptionMenu(self, self.dataset_var, \"Dataset 1\", \"Dataset 2\", command=self.load_dataset)\n",
    "        self.dataset_menu.pack(side=tk.TOP, anchor=tk.NE)\n",
    "\n",
    "        self.clean_button = tk.Button(self, text=\"Clean and Represent Data\", command=self.clean_and_represent_data)\n",
    "        self.clean_button.pack()\n",
    "\n",
    "        self.index_button = tk.Button(self, text=\"Index Data\", command=self.index_data)\n",
    "        self.index_button.pack()\n",
    "\n",
    "        self.calculate_metrics_button = tk.Button(self, text=\"Calculate Overall Metrics\", command=self.calculate_overall_metrics)\n",
    "        self.calculate_metrics_button.pack()\n",
    "\n",
    "        self.vectorizer = None\n",
    "        self.tfidf_matrix = None\n",
    "        self.ids_data = None\n",
    "         #object from class TextCleaning\n",
    "        self.cleaning_class = TextCleaning()\n",
    "#path a data set \n",
    "    def load_dataset(self, dataset_choice):\n",
    "        if dataset_choice == \"Dataset 1\":\n",
    "            self.input_path = 'C:\\\\Users\\\\MJ\\\\Desktop\\\\IR\\\\lotte\\\\lifestyle\\\\dev\\\\qas.search.jsonl'\n",
    "        elif dataset_choice == \"Dataset 2\":\n",
    "            self.input_path = 'C:\\\\Users\\\\MJ\\\\Desktop\\\\IR\\\\webis-touche2020\\\\queries.jsonl'\n",
    "        else:\n",
    "            return\n",
    "\n",
    "        self.results_text.insert(tk.INSERT, f\"Selected {dataset_choice}\\n\")\n",
    "\n",
    "        self.queries_searched = []\n",
    "        self.precision_list = []\n",
    "        self.recall_list = []\n",
    "        self.average_precision_list = []\n",
    "        self.reciprocal_rank_list = []\n",
    "        relevant_docs.clear()\n",
    "#if insert data set befor  insert query\n",
    "    def clean_and_represent_data(self):\n",
    "        if not hasattr(self, 'input_path') or not self.input_path:\n",
    "            self.results_text.insert(tk.INSERT, \"Please select a dataset first.\\n\")\n",
    "            return\n",
    "\n",
    "        cleaned_data, raw_data = load_and_clean_data(self.input_path)\n",
    "#if cleaning before  insert query\n",
    "        if not cleaned_data:\n",
    "            self.results_text.insert(tk.INSERT, \"Loaded dataset is empty after cleaning.\\n\")\n",
    "            return\n",
    "\n",
    "        corpus_data = cleaned_data\n",
    "#index Dataset 1\n",
    "        final_results = []\n",
    "\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        vectorizer.fit(corpus_data)\n",
    "\n",
    "        for doc_id, doc_val in enumerate(corpus_data):\n",
    "            vector = vectorizer.transform([doc_val]).toarray()\n",
    "            distribution = vector[0]\n",
    "            if not all(v == 0 for v in distribution):\n",
    "                final_results.append({\n",
    "                    \"id_doc\": doc_id,\n",
    "                    \"vector_doc\": vector.tolist(),\n",
    "                    \"distribution_doc\": distribution.tolist()\n",
    "                })\n",
    "\n",
    "        path_of_outer_dir = 'C:/Users/MJ/Desktop/IR'\n",
    "\n",
    "        if not os.path.exists(path_of_outer_dir):\n",
    "            os.makedirs(path_of_outer_dir)\n",
    "\n",
    "        output_file_path = os.path.join(path_of_outer_dir, 'processed_data.json')\n",
    "\n",
    "        with open(output_file_path, 'w') as output_file:\n",
    "            json.dump(final_results, output_file, indent=4)\n",
    "\n",
    "        self.results_text.insert(tk.INSERT, \"Data cleaned and represented successfully.\\n\")\n",
    "        self.results_text.insert(tk.INSERT, f\"Saved to {output_file_path}\\n\")\n",
    "\n",
    "        self.vectorizer = vectorizer\n",
    "        self.tfidf_matrix = vectorizer.transform(corpus_data)\n",
    "        self.ids_data = [json.loads(line).get('_id', f\"document{i}\") for i, line in enumerate(raw_data)]\n",
    "#index Dataset 2\n",
    "\n",
    "    def index_data(self):\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            self.results_text.insert(tk.INSERT, \"Please clean and represent the data first.\\n\")\n",
    "            return\n",
    "#store index dataset 2\n",
    "        index_dir = 'C:/Users/MJ/Desktop/IR/fileindex'\n",
    "        if not os.path.exists(index_dir):\n",
    "            os.makedirs(index_dir)\n",
    "\n",
    "        index_file_path = os.path.join(index_dir, 'index.json')\n",
    "\n",
    "        index_data = {\n",
    "            \"ids_data\": self.ids_data,\n",
    "            \"tfidf_matrix\": self.tfidf_matrix.toarray().tolist()\n",
    "        }\n",
    "\n",
    "        with open(index_file_path, 'w') as index_file:\n",
    "            json.dump(index_data, index_file, indent=4)\n",
    "\n",
    "        self.results_text.insert(tk.INSERT, \"Data indexed successfully.\\n\")\n",
    "        self.results_text.insert(tk.INSERT, f\"Saved to {index_file_path}\\n\")\n",
    "\n",
    "        self.queries_searched = []\n",
    "        self.precision_list = []\n",
    "        self.recall_list = []\n",
    "        self.average_precision_list = []\n",
    "        self.reciprocal_rank_list = []\n",
    "        relevant_docs.clear()\n",
    "\n",
    "    def search(self):\n",
    "        if self.vectorizer is None or self.tfidf_matrix is None:\n",
    "            self.results_text.insert(tk.INSERT, \"Please load and clean a dataset first.\\n\")\n",
    "            return\n",
    "\n",
    "        query = self.query_entry.get()\n",
    "        self.queries_searched.append(query)\n",
    "#Matching the query and ranking the results\n",
    "        cleaned_query = self.cleaning_class.clean_text(query)\n",
    "        query_vector = self.vectorizer.transform([cleaned_query])\n",
    "\n",
    "        scores = cosine_similarity(query_vector, self.tfidf_matrix).flatten()\n",
    "        sorted_indices = np.argsort(scores)[::-1]\n",
    "\n",
    "        filtered_results = [(self.ids_data[idx], scores[idx]) for idx in sorted_indices if scores[idx] > 0.0]\n",
    "        top_10_results = filtered_results[:10]\n",
    "        results = [doc_id for doc_id, score in top_10_results]\n",
    "\n",
    "        if query not in relevant_docs and results:\n",
    "            relevant_docs[query] = results\n",
    "\n",
    "        relevant_documents = relevant_docs.get(query, [])\n",
    "\n",
    "        precision_at_10 = sum(1 for doc_id in results if doc_id in relevant_documents) / 10\n",
    "        self.precision_list.append(precision_at_10)\n",
    "\n",
    "        recall_at_10 = sum(1 for doc_id in results if doc_id in relevant_documents) / len(relevant_documents) if relevant_documents else 0\n",
    "        self.recall_list.append(recall_at_10)\n",
    "\n",
    "        relevant_count = 0\n",
    "        average_precision_at_10 = 0\n",
    "        for i, doc_id in enumerate(results, 1):\n",
    "            if doc_id in relevant_documents:\n",
    "                relevant_count += 1\n",
    "                average_precision_at_10 += relevant_count / i\n",
    "        average_precision_at_10 /= min(len(relevant_documents), 10) if relevant_documents else 1\n",
    "        self.average_precision_list.append(average_precision_at_10)\n",
    "\n",
    "        reciprocal_rank = 0\n",
    "        for i, doc_id in enumerate(results, 1):\n",
    "            if doc_id in relevant_documents:\n",
    "                reciprocal_rank = 1 / i\n",
    "                break\n",
    "        self.reciprocal_rank_list.append(reciprocal_rank)\n",
    "\n",
    "        self.results_text.delete(1.0, tk.END)\n",
    "        self.results_text.insert(tk.INSERT, \"Top 10 Relevant Documents:\\n\")\n",
    "        self.results_text.insert(tk.INSERT, \"\\n\".join([f\"{doc_id}: {score:.4f}\" for doc_id, score in top_10_results]))\n",
    "\n",
    "        self.metrics_text.delete(1.0, tk.END)\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Precision@10: {precision_at_10}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Recall@10: {recall_at_10}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Average Precision@10: {average_precision_at_10}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Reciprocal Rank@10: {reciprocal_rank}\\n\")\n",
    "\n",
    "    def get_suggestions(self):\n",
    "        if not hasattr(self, 'input_path') or not self.input_path:\n",
    "            self.results_text.insert(tk.INSERT, \"Please select a dataset first.\\n\")\n",
    "            return\n",
    "\n",
    "        query = self.query_entry.get()\n",
    "        data = pd.read_json(self.input_path, lines=True)\n",
    "        suggestions = difflib.get_close_matches(query, data['query'].tolist(), n=3)\n",
    "\n",
    "        self.results_text.delete(1.0, tk.END)\n",
    "        if suggestions:\n",
    "            self.results_text.insert(tk.INSERT, \"Suggestions:\\n\")\n",
    "            self.results_text.insert(tk.INSERT, \"\\n\".join(suggestions))\n",
    "\n",
    "            overall_precision_before = np.mean([p for p in self.precision_list if p > 0])\n",
    "            overall_recall_before = np.mean([r for r in self.recall_list if r > 0])\n",
    "            overall_avg_precision_before = np.mean([ap for ap in self.average_precision_list if ap > 0])\n",
    "            overall_reciprocal_rank_before = np.mean([rr for rr in self.reciprocal_rank_list if rr > 0])\n",
    "\n",
    "            self.results_text.insert(tk.INSERT, f\"\\n\\nOverall Metrics Before Suggestions:\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Precision@10: {overall_precision_before}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Recall@10: {overall_recall_before}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Average Precision@10: {overall_avg_precision_before}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Reciprocal Rank@10: {overall_reciprocal_rank_before}\\n\")\n",
    "\n",
    "            for suggestion in suggestions:\n",
    "                self.query_entry.delete(0, tk.END)\n",
    "                self.query_entry.insert(0, suggestion)\n",
    "                self.search()\n",
    "\n",
    "            overall_precision_after = np.mean([p for p in self.precision_list if p > 0])\n",
    "            overall_recall_after = np.mean([r for r in self.recall_list if r > 0])\n",
    "            overall_avg_precision_after = np.mean([ap for ap in self.average_precision_list if ap > 0])\n",
    "            overall_reciprocal_rank_after = np.mean([rr for rr in self.reciprocal_rank_list if rr > 0])\n",
    "\n",
    "            self.results_text.insert(tk.INSERT, f\"\\n\\nOverall Metrics After Suggestions:\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Precision@10: {overall_precision_after}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Recall@10: {overall_recall_after}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Average Precision@10: {overall_avg_precision_after}\\n\")\n",
    "            self.results_text.insert(tk.INSERT, f\"Overall Reciprocal Rank@10: {overall_reciprocal_rank_after}\\n\")\n",
    "        else:\n",
    "            self.results_text.insert(tk.INSERT, \"No suggestions found.\\n\")\n",
    "#calculation rate\n",
    "    def calculate_overall_metrics(self):\n",
    "        precision_values = [p for p in self.precision_list if p > 0]\n",
    "        recall_values = [r for r in self.recall_list if r > 0]\n",
    "        avg_precision_values = [ap for ap in self.average_precision_list if ap > 0]\n",
    "        reciprocal_rank_values = [rr for rr in self.reciprocal_rank_list if rr > 0]\n",
    "\n",
    "        if not precision_values or not recall_values or not avg_precision_values or not reciprocal_rank_values:\n",
    "            self.metrics_text.insert(tk.INSERT, \"No search queries executed yet.\\n\")\n",
    "            return\n",
    "\n",
    "        overall_precision = np.mean(precision_values)\n",
    "        overall_recall = np.mean(recall_values)\n",
    "        overall_avg_precision = np.mean(avg_precision_values)\n",
    "        overall_reciprocal_rank = np.mean(reciprocal_rank_values)\n",
    "#print rate\n",
    "        self.metrics_text.insert(tk.INSERT, \"Overall Metrics:\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Overall Precision@10: {overall_precision}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Overall Recall@10: {overall_recall}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Overall Average Precision@10: {overall_avg_precision}\\n\")\n",
    "        self.metrics_text.insert(tk.INSERT, f\"Overall Reciprocal Rank@10: {overall_reciprocal_rank}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = Application()\n",
    "    app.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff662c42-dad5-4d0f-9e2c-025691d369b9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
